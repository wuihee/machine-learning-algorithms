{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Notation\n",
    "\n",
    "- $x$ - Input features.\n",
    "- $x_j$ - The $j^{th}$ feature.\n",
    "- $\\vec{x^{(i)}}$ - Features of the $i^{th}$ training example; the $i^{th}$ row.\n",
    "- $x_j^{(i)}$ - The $i^{th}$ element for the $j^{th}$ feature.\n",
    "- $y$ - Output/target variable.\n",
    "- $y^{(i)}$ - The $i^{th}$ output value.\n",
    "- $m$ - Number of training examples.\n",
    "- $n$ - Number of training features.\n",
    "- $\\vec{w}, b$ - Model parameters.\n",
    "- $\\alpha$ - Learning rate.\n",
    "\n",
    "## Formulas\n",
    "\n",
    "### Sigmoid Function / Model Prediction\n",
    "\n",
    "$g(z) = \\frac{1}{1 + e^{-z}}$, where\\\n",
    "$z = f_{\\vec{w}, b}(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b$\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "$L(f_{\\vec{w}, b}(\\vec{x^{(i)}}), y^{(i)}) = $\\\n",
    "$-\\log (f*{\\vec{w}, b}(\\vec{x^{(i)}}))$ if $y^{(i)} = 1$\\\n",
    "$-\\log (1 - f*{\\vec{w}, b}(\\vec{x^{(i)}}))$ if $y^{(i)} = 0$\n",
    "\n",
    "Which simplifies to:\n",
    "\n",
    "$L(f_{\\vec{w}, b}(\\vec{x^{(i)}}), y^{(i)}) = - y^{(i)} \\log (f_{\\vec{w}, b}(\\vec{x^{(i)}})) - (1 - y^{(i)}) \\log (1 - f_{\\vec{w}, b}(\\vec{x^{(i)}}))$\n",
    "\n",
    "Basically, if your target value, $y^{(i)} = 1$, we're gonna punish you by making $L -> infinity$ as you go to 0, and vice-versa.\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "$J(\\vec{w}, b) = \\frac{1}{2m} \\sum_{i = 1}^{m} L(f_{\\vec{w}, b}(\\vec{x^{(i)}}) - y^{(i)})$\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Repeat the until convergence:\n",
    "\n",
    "$w_j = w_j - \\alpha \\frac{\\partial}{\\partial w_j} J(\\vec{w}, b)$\\\n",
    "$b = b - \\alpha \\frac{\\partial}{\\partial b} J(\\vec{w}, b)$\n",
    "\n",
    "Repeat the until convergence:\n",
    "\n",
    "$w_j =  w_j - \\alpha [\\frac{1}{m} \\sum_{i = 1}^{m}(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)})x_j^{(i)}]$\\\n",
    "$b = b - \\alpha [\\frac{1}{m} \\sum_{i = 1}^{m}(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)})]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.001, n_iters=1000, threshold=0.5):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.threshold = threshold\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Gradient Descent\n",
    "        for _ in range(self.n_iters):\n",
    "            y_predict = self._sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, y_predict - y)\n",
    "            db = (1 / n_samples) * sum(y_predict - y)\n",
    "\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_predict = self._sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "        return [1 if i > self.threshold else 0 for i in y_predict]\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:, mean radius, mean texture, mean perimeter, mean area, mean smoothness, mean compactness, mean concavity, mean concave points, mean symmetry, mean fractal dimension, radius error, texture error, perimeter error, area error, smoothness error, compactness error, concavity error, concave points error, symmetry error, fractal dimension error, worst radius, worst texture, worst perimeter, worst area, worst smoothness, worst compactness, worst concavity, worst concave points, worst symmetry, worst fractal dimension\n",
      "X_train shape: (113, 30), y-train shape: (113,)\n",
      "X_test shape: (456, 30), y-test shape: (456,)\n"
     ]
    }
   ],
   "source": [
    "breast_cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    breast_cancer.data, breast_cancer.target, train_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Features:\", *breast_cancer.feature_names, sep=\", \")\n",
    "print(f\"X_train shape: {X_train.shape}, y-train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y-test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    return np.sum(y_pred == y_test) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 0.8141592920353983\n",
      "Test Set Accuracy: 0.8026315789473685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wuihee\\AppData\\Local\\Temp\\ipykernel_100664\\436757810.py:29: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "train_pred = clf.predict(X_train)\n",
    "test_pred = clf.predict(X_test)\n",
    "print(f\"Train Set Accuracy: {accuracy(train_pred, y_train)}\")\n",
    "print(f\"Test Set Accuracy: {accuracy(test_pred, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
