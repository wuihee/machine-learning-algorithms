{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Notation\n",
    "\n",
    "- $x$ - Input features.\n",
    "- $x_j$ - The $j^{th}$ feature.\n",
    "- $\\vec{x^{(i)}}$ - Features of the $i^{th}$ training example; the $i^{th}$ row.\n",
    "- $x_j^{(i)}$ - The $i^{th}$ element for the $j^{th}$ feature.\n",
    "- $y$ - Output/target variable.\n",
    "- $y^{(i)}$ - The $i^{th}$ output value.\n",
    "- $m$ - Number of training examples.\n",
    "- $n$ - Number of training features.\n",
    "- $\\vec{w}, b$ - Model parameters.\n",
    "- $\\alpha$ - Learning rate.\n",
    "\n",
    "## Formulas\n",
    "\n",
    "### Sigmoid Function / Model Prediction\n",
    "\n",
    "$g(z) = \\frac{1}{1 + e^{-z}}$, where\\\n",
    "$z = f_{\\vec{w}, b}(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b$\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "$L(f_{\\vec{w}, b}(\\vec{x^{(i)}}), y^{(i)}) = $\\\n",
    "$-\\log (f_{\\vec{w}, b}(\\vec{x^{(i)}}))$ if $y^{(i)} = 1$\\\n",
    "$-\\log (1 - f_{\\vec{w}, b}(\\vec{x^{(i)}}))$ if $y^{(i)} = 0$\\\n",
    "\n",
    "Which simplifies to:\n",
    "\n",
    "$L(f_{\\vec{w}, b}(\\vec{x^{(i)}}), y^{(i)}) = - y^{(i)} \\log (f_{\\vec{w}, b}(\\vec{x^{(i)}})) - (1 - y^{(i)}) \\log (1 - f_{\\vec{w}, b}(\\vec{x^{(i)}}))$\n",
    "\n",
    "Basically, if your target value, $y^{(i)} = 1$, we're gonna punish you by making $L -> infinity$ as you go to 0, and vice-versa.\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "$J(\\vec{w}, b) = \\frac{1}{2m} \\sum_{i = 1}^{m} L(f_{\\vec{w}, b}(\\vec{x^{(i)}}) - y^{(i)})$\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Repeat the until convergence:\n",
    "\n",
    "$w_j = w_j - \\alpha \\frac{\\partial}{\\partial w_j} J(\\vec{w}, b)$\\\n",
    "$b = b - \\alpha \\frac{\\partial}{\\partial b} J(\\vec{w}, b)$\n",
    "\n",
    "Repeat the until convergence:\n",
    "\n",
    "$w_j =  w_j - \\alpha [\\frac{1}{m} \\sum_{i = 1}^{m}(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)})x_j^{(i)}]$\\\n",
    "$b = b - \\alpha [\\frac{1}{m} \\sum_{i = 1}^{m}(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)})]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.001, n_iters=1000, threshold=0.5):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.threshold = threshold\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Gradient Descent\n",
    "        for _ in range(self.n_iters):\n",
    "            y_predict = self._sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, y_predict - y)\n",
    "            db = (1 / n_samples) * sum(y_predict - y)\n",
    "\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_predict = self._sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "        return [1 if i > self.threshold else 0 for i in y_predict]\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Visualize Random Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of informative, redundant and repeated features must sum to less than the number of total features",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mmake_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y)\n\u001b[1;32m      4\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/mlagos-lnatBVRd-py3.11/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/mlagos-lnatBVRd-py3.11/lib/python3.11/site-packages/sklearn/datasets/_samples_generator.py:214\u001b[0m, in \u001b[0;36mmake_classification\u001b[0;34m(n_samples, n_features, n_informative, n_redundant, n_repeated, n_classes, n_clusters_per_class, weights, flip_y, class_sep, hypercube, shift, scale, shuffle, random_state)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Count features, clusters and samples\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_informative \u001b[38;5;241m+\u001b[39m n_redundant \u001b[38;5;241m+\u001b[39m n_repeated \u001b[38;5;241m>\u001b[39m n_features:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of informative, redundant and repeated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures must sum to less than the number of total\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Use log2 to avoid overflow errors\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_informative \u001b[38;5;241m<\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2(n_classes \u001b[38;5;241m*\u001b[39m n_clusters_per_class):\n",
      "\u001b[0;31mValueError\u001b[0m: Number of informative, redundant and repeated features must sum to less than the number of total features"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=100, n_features=1)\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.scatter(X[:, 0], y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagos-lnatBVRd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
