{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "## Notation\n",
    "\n",
    "- $p_1$ - The fraction of correctly classified labels on a node in the decision tree.\n",
    "- $p_0 = 1 - p_1$ - The fraction of incorrectly classified labels on a node in the decision tree.\n",
    "- $p_1^{\\text{left}}$, $p_1^{\\text{right}}$ - Fraction of positive (correct) labels on the left or right subtree respectively.\n",
    "- $w^{\\text{left}}$, $w^{\\text{right}}$ - Fraction of all examples that went to the left or right subtree respectively.\n",
    "- $H(p_1)$ - The *entropy* on a node in the decision tree. Entropy measures the homogeneity of the node.\n",
    "\n",
    "## Formulas\n",
    "\n",
    "### Entropy\n",
    "\n",
    "$H(p_1) = -p_1 \\log_2(p_1) - p_0 \\log_2(p_0)$\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "Information gain refers to the reduction in entropy from making a split.\n",
    "\n",
    "$H(p_1^{\\text{root}}) - (w^{\\text{left}} H(p_1^{\\text{left}}) + w^{\\text{right}} H(p_1^{\\text{right}}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth) -> None:\n",
    "        self.tree = []\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def _build_tree(self, X, y, indices, current_depth):\n",
    "        if current_depth == self.max_depth:\n",
    "            return\n",
    "        \n",
    "        best_feature = self._get_best_split(X, y, indices)\n",
    "        left_indices, right_indices = self._split_node(X, indices, best_feature)\n",
    "        self.tree.append((left_indices, right_indices, best_feature))\n",
    "        self._build_tree(X, y, left_indices, self.max_depth, current_depth + 1)\n",
    "        self._build_tree(X, y, right_indices, self.max_depth, current_depth + 1)\n",
    "\n",
    "    def _get_best_split(self, X, y, indices):\n",
    "        n_features = X.shape[1]\n",
    "        best_feature = -1\n",
    "\n",
    "        max_gain = 0\n",
    "        for feature in range(n_features):\n",
    "            information_gain = self._information_gain(X, y, indices)\n",
    "            if information_gain > max_gain:\n",
    "                max_gain = information_gain\n",
    "                best_feature = feature\n",
    "        \n",
    "        return best_feature\n",
    "\n",
    "    def _information_gain(self, X, y, indices, feature):\n",
    "        left_indices, right_indices = self._split_node(X, indices, feature)\n",
    "        X_node, y_node = X[indices], y[indices]\n",
    "        X_left, y_left = X[left_indices], y[left_indices]\n",
    "        X_right, y_right = X[right_indices], y[right_indices]\n",
    "\n",
    "        entropy_root = self._entropy(y_node)\n",
    "        entropy_left = self._entropy(y_left)\n",
    "        entropy_right = self._entropy(y_right)\n",
    "        w_left = len(X_left) / len(X_node)\n",
    "        w_right = len(X_right) / len(X_node)\n",
    "\n",
    "        return entropy_root - (w_left * entropy_left + w_right * entropy_right)\n",
    "\n",
    "    def _split_node(\n",
    "        X: np.ndarray, indices: list, feature: int\n",
    "    ) -> tuple[list[int], list[int]]:\n",
    "        \"\"\"\n",
    "        Splits the data at the given node into left and right branches.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Data matrix of shape (n_samples, n_features).\n",
    "            indices (list): List containing active indices.\n",
    "            feature (int): Index to split feature on.\n",
    "\n",
    "        Returns:\n",
    "            tuple[list[int], list[int]]: _description_\n",
    "        \"\"\"\n",
    "        left_indices = []\n",
    "        right_indices = []\n",
    "\n",
    "        for i in indices:\n",
    "            if X[i, feature] == 1:\n",
    "                left_indices.append(i)\n",
    "            else:\n",
    "                right_indices.append(i)\n",
    "\n",
    "        return left_indices, right_indices\n",
    "\n",
    "    def _entropy(self, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Computes the entropy for a given node.\n",
    "\n",
    "        Args:\n",
    "            y (np.ndarray): NumPy array indicating whether each example at\n",
    "                            a given node is positive (1) or negative (0).\n",
    "\n",
    "        Returns:\n",
    "            float: Entropy value.\n",
    "        \"\"\"\n",
    "        # If all examples are either positive or negative, entropy is 0.\n",
    "        if sum(y) == 0 or sum(y) == len(y):\n",
    "            return 0\n",
    "\n",
    "        p1 = sum(y) / len(y)\n",
    "        p0 = 1 - p1\n",
    "        return -p1 * np.log2(p1) - p0 * np.log2(p0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
